# 1.3 基础DataStream API

## Data types <a href="#j8uwn" id="j8uwn"></a>

序列化在分布式系统中至关重要；这是不同机器上的进程能够相互共享数据的基础。

* DataSet 和 DataStream API 共享相同的类型系统
* Flink 有自己的序列化器，主要用于
  * 基础类型
    * String, Long, Integer, Boolean, …
    * Arrays
  * 复合类型
    * Tuples
    * POJOs
    * Scala Case Classes
* 其他情况下，Flink 回退到 Kryo 进行 ser/de.
* 也可以采用其他Flink支持的序列化器，如Avro。

### Tuples <a href="#rifbm" id="rifbm"></a>

Scala: 采用默认的Scala tuples (1 to 22 fields)

Java: Flink实现了Tuple1到Tuple25

```
Tuple2<String, Integer> person = 
    new Tuple2<>("Max Mustermann", 42);

// zero based index!
String name = person.f0;
Integer age = person.f1;
```

### POJOs <a href="#s3scr" id="s3scr"></a>

任意一个Java类，满足：

* 有一个空的构造函数
* 可被公开访问的属性（public 属性或者默认的getter/setter）

```
public class Person {
    public String name;
    public Integer age;
    public Person() {};
    public Person(String name, Integer age) {…};
}

DataStream<Person> p = env.fromElements(new Person("Bob", 65));
```

从 Flink 1.8 开始，Flink 将负责schema演变和state迁移，以便对使用 Flink 内置序列化器序列化的 POJO 进行直接更改。

### Case classes (Scala) <a href="#u9qm4" id="u9qm4"></a>

Scala case classes是原生支持的

```
case class Person(name: String, age: Int)
d: DataStream[Person] = 
     env.fromElements(Person("Bob", 65))`
```

目前（尚）不支持 Scala case class的状态演化。见 [FLINK-10896](https://issues.apache.org/jira/browse/FLINK-10896), 目前技术可行性还待验证。





## DataStream API: 基础算子 <a href="#tfvyj" id="tfvyj"></a>

在无界流上持续运行的计算

![](<../../.gitbook/assets/image (7).png>)

在Flink流式计算简介一节中，提到 Flink 提供了一次一个事件的流处理。 现在具体看看：在“your code”的地方可以做什么？ 总结来说，就是transforming。



## Transformations: Filter <a href="#nj2jb" id="nj2jb"></a>

```
public static void main(String[] args) throws Exception {
    final StreamExecutionEnvironment env =
        StreamExecutionEnvironment.getExecutionEnvironment();

    DataStream<Integer> integers = env.fromElements(1, 2, 3, 4, 5);
    
    DataStream<Integer> odds = integers
        .filter(new FilterFunction<Integer>() {
            @Override
            public boolean filter(Integer value) {
                return ((value % 2) == 1);
            }
        });
    // java 8
    // DataStream<Integer> odds = integers.filter(x -> (x%2 ==1) )

    odds.print();

    env.execute();
}

> 1, 3, 5
```

以上是一个完整的Flink应用的实际例子，除了filter以外，还可以关注下

```
StreamingExecutionEnvironment env
print()
execute()
```

### Transformations: Map <a href="#hrhlu" id="hrhlu"></a>

```
DataStream<Integer> integers = env.fromElements(1, 2, 3, 4, 5);

DataStream<Integer> doubleOdds = integers
    .filter(new FilterFunction<Integer>() {
        @Override
        public boolean filter(Integer value) {
            return ((value % 2) == 1);
        }
    })
    .map(new MapFunction<Integer, Integer>() {
        @Override
        public Integer map(Integer value) {
            return value * 2;
        }
    });
    
// java 8
//DataStream<Integer> doubleOdds = integers.filter(x -> (x%2==1) )
                   .map( x -> x*2)

doubleOdds.print();

> 2, 6, 10
```

注意FilterFunction和MapFunction的接口声明，参数不同。虽然都是对单元素处理，MapFunction中第1个接受输入、第2个接受输出。

### Transformations: FlatMap <a href="#dtfm5" id="dtfm5"></a>

```
DataStream<Integer> integers = env.fromElements(1, 2, 3, 4, 5);

DataStream<Integer> doubleOdds = integers
    .flatMap(new FlatMapFunction<Integer, Integer>() {
        @Override
        public void flatMap(Integer value, Collector<Integer> out) {
            if ((value % 2) == 1) {
                out.collect(value * 2);
            }
        }
    });

doubleOdds.print();
```

注意，这里FlatMapFunction interface也是接收2个参数，一个是输入类型，一个是输出类型。里面flatMap函数返回类型为void，同时值得注意的是函数的第2个参数是Collector类型。

flatMap 在函数必须处理无法解析的输入或其他错误情况时很有用。或者当单个输入记录可能产生多个输出记录时，例如，如果它需要解包打平一个数组。



在map函数中由于 `OUT` 是 `Integer` 而不是泛型，所以 Flink 可以从方法签名 `OUT map(IN value)` 的实现中自动提取出结果的类型信息。

不幸的是，像 `flatMap()` 这样的函数，它的签名 `void flatMap(IN value, Collector<OUT> out)` 被 Java 编译器编译为 `void flatMap(IN value, Collector out)`。这样 Flink 就无法自动推断输出的类型信息了。

Flink 很可能抛出如下异常：

```
org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of 'Collector' are missing.
    In many cases lambda methods don't provide enough information for automatic type extraction when Java generics are involved.
    An easy workaround is to use an (anonymous) class instead that implements the 'org.apache.flink.api.common.functions.FlatMapFunction' interface.
    Otherwise the type has to be specified explicitly using type information.
```

```

import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

public class TransformationFlatMapExp {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<Integer> source = env.fromElements(1,2,3);

        // 必须声明 collector 类型
        source.flatMap((Integer number, Collector<String> out) -> {
                    StringBuilder builder = new StringBuilder();
                    for(int i = 0; i < number; i++) {
                        builder.append("a");
                        out.collect(builder.toString());
                    }
                })
                //显式提供类型信息
                .returns(Types.STRING)
                // 打印 "a", "a", "aa", "a", "aa", "aaa"
                .print();

        env.execute();
    }
}
```

实际ide打印结果如下，按照前面编号数字的提示，可以推断出上述代码注释中的顺序。但是print不会完全按照执行顺序打印。

![](<../../.gitbook/assets/image (11).png>)

{% hint style="warning" %}
debug没调试出来注释效果
{% endhint %}

### Distributed stream processing: keyBy

![](<../../.gitbook/assets/image (10).png>)

* 跨多个实例分布计算
* 键是根据每个流元素计算的
* 对数据进行分区——具有相同键的所有元素将由相同的算子实例处理
* 特定算子是key-aware感知的，例如 reduce 和 window
* Flink 管理每个key的state和timers

#### 实现一个KeySelector <a href="#y64pu" id="y64pu"></a>

```
keyedSensorReadings = sensorReadings.keyBy(r -> r.id)
```

#### 有效的key类型 <a href="#dhmkv" id="dhmkv"></a>

* key由KeySelector方法从每一个stream消息中计算而来。
  * KeySelector必须是确定性的
  * 它计算的key必须具有 hashCode() 和 equals() 的有效实现
  * 数组和枚举类型不能作为key。因为它们的 hashCode() 实现在 JVM 之间不是确定性的
* 复合类型也可以作为key
  * 所有字段必须是key类型
  * 嵌套字段也可以用作key

### 一些tips

* 使用 env.fromElements(...) 或 env.fromCollection(...) 快速创建 DataStream 以进行试验
* 使用 print() 打印数据流，这是非常有效的调试手段。
* Lazy execution会使调试变得棘手，但可以在 IDE 中使用断点
